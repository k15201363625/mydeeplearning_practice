{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image # 更方便读取存储图像\n",
    "import tensorflow as tf\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取参数 构建VGGNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' VGGNET 要求的输入图像归一化参数 '''\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "class VGGNet:\n",
    "    \"\"\"\n",
    "    load params from vgg16.npy (pre-train models)\n",
    "    data is dict form \n",
    "    need build net structure and then give params\n",
    "    so need constant/trainable = False layers params\n",
    "    so need to use tf.nn.conv2d to assign constant value for params\n",
    "    (because of tf.layers.conv2d 自动生成变量 赋值不方便) \n",
    "    tf.nn下的fun 比tf.layers的fun少了一层封装 不方便但是更灵活\n",
    "    build vgg-16 net structure\n",
    "    \"\"\"\n",
    "    def __init__(self,data_dict):\n",
    "        self.data_dict = data_dict\n",
    "    '''获取参数'''\n",
    "    def _get_conv_layer_weights(self,name):\n",
    "        return tf.constant(self.data_dict[name][0],name='conv_w')\n",
    "    def _get_fc_layer_weights(self,name):\n",
    "        return tf.constant(self.data_dict[name][0],name='fc_w')\n",
    "    def _get_bias(self,name):\n",
    "        return tf.constant(self.data_dict[name][1],name='b')\n",
    "    '''构建layer'''\n",
    "    def conv_layer(self,x,name):\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self._get_conv_layer_weights(name)\n",
    "            conv_b = self._get_bias(name)\n",
    "            conv = tf.nn.conv2d(x,conv_w,[1,1,1,1],padding='SAME')\n",
    "            conv = tf.nn.bias_add(conv,conv_b)\n",
    "            conv = tf.nn.relu(conv)\n",
    "            return conv\n",
    "    def pooling_layer(self,x,name):\n",
    "        with tf.name_scope(name):\n",
    "            return tf.nn.max_pool(x,ksize = [1,2,2,1],\n",
    "                                  strides = [1,2,2,1],\n",
    "                                  padding='SAME',name = name)\n",
    "    def fc_layer(self,x,name,activation=tf.nn.relu):\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self._get_fc_layer_weights(name)\n",
    "            fc_b = self._get_bias(name)\n",
    "            h = tf.matmul(x,fc_w)\n",
    "            h = tf.nn.bias_add(h,fc_b)\n",
    "            if activation == None:\n",
    "                '''最后一层不需要激活 使用softmax即可'''\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "    def flatten_layer(self,x,name):\n",
    "        \"\"\"flatten layer(tf.layers.flatten也可以)\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            x_shape = x.get_shape().as_list()[1:]\n",
    "            dim2 = 1\n",
    "            for i in x_shape:\n",
    "                dim2 *= i\n",
    "            x = tf.reshape(x,[-1,dim2])\n",
    "            return x\n",
    "    \n",
    "    def build(self,x_rgb):\n",
    "        \"\"\"\n",
    "        给定输入rgb格式224*224 转换成bgr 并且用均值归一化\n",
    "        实现VGG16输入预处理 VGG16输入为224*224 的抽样预处理图像\n",
    "        并对构件时间进行统计\n",
    "        \"\"\"\n",
    "        assert x_rgb.get_shape().as_list()[1:] == [224,224,3]\n",
    "        start_time = time.time()\n",
    "        print('building start')\n",
    "        '''prepocessing by split and concat'''\n",
    "        r,g,b = tf.split(x_rgb,[1,1,1],axis=3)\n",
    "        x_bgr = tf.concat([b-VGG_MEAN[0],\n",
    "                           g-VGG_MEAN[1],\n",
    "                           r-VGG_MEAN[2]],axis=3)\n",
    "        \n",
    "        '''build network structure with five conv layers and three fc layers'''\n",
    "        '''\n",
    "        为了方便下面使用 所以用成员variables 从而方便外部获取 调用\n",
    "        '''\n",
    "        self.conv1_1 = self.conv_layer(x_bgr,'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1,'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2,'pool1')\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1,'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1,'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2,'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2,'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1,'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2,'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3,'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3,'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1,'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2,'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3,'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4,'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1,'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2,'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3,'pool5')\n",
    "        \n",
    "        '''fc layer 在图像风格转换中农意义不大 '''\n",
    "        '''节省5s\n",
    "        self.flatten5 = self.flatten_layer(self.pool5,'flatten5')\n",
    "        self.fc6 = self.fc_layer(self.flatten5,'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6,'fc7')\n",
    "        self.fc8 = self.fc_layer(self.fc7,'fc8',activation=None)\n",
    "        self.prob = tf.nn.softmax(self.fc8,name = 'softmax')\n",
    "        '''\n",
    "        print('building stop,total time spends: %4ds'% (time.time()-start_time))\n",
    "        # return self.prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper params defination and file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_npy_path = './style_transfer_data/vgg16.npy'\n",
    "content_img_path = './style_transfer_data/content.jpg'\n",
    "style_img_path = './style_transfer_data/style.jpg'\n",
    "\n",
    "out_dir = './style_transfer_data/run_out'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    \n",
    "'''loss weights\n",
    "二者相差甚远 所以通过不同权重使二者相近\n",
    "'''\n",
    "lambda_content = 0.1\n",
    "lambda_style = 500\n",
    "\n",
    "'''\n",
    "由于只有一个variables需要修正 深层网络 使用adam 指定训练次数 以及较大的初始学习率\n",
    "'''\n",
    "\n",
    "num_steps = 100\n",
    "learning_rate = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building start\n",
      "building stop,total time spends:    0s\n",
      "building start\n",
      "building stop,total time spends:    0s\n",
      "building start\n",
      "building stop,total time spends:    0s\n"
     ]
    }
   ],
   "source": [
    "def read_img(img_name):\n",
    "    img = Image.open(img_name)\n",
    "    img_np = np.array(img)\n",
    "    # 维度转换\n",
    "    img_np = np.asarray([img_np],dtype = np.int32)\n",
    "#     img_np = np.reshape(img_np,[1,224,224,3])\n",
    "    return img_np\n",
    "\n",
    "def initial_result_img(shape,mean,stddev):\n",
    "    \"\"\"\n",
    "    特别小心 不同与其他图片使用placeholder传入 是data 不是变量\n",
    "    这里的result_img 作为输入 是额唯一的起始变量 所以需要声明 \n",
    "    否则整个网络中全都是常量\n",
    "    \"\"\"\n",
    "    initial_img = tf.truncated_normal(shape,mean=mean,stddev=stddev)\n",
    "    return tf.Variable(initial_img)\n",
    "\n",
    "result_img = initial_result_img([1,224,224,3],127.5,12)\n",
    "content_img_val = read_img(content_img_path)\n",
    "style_img_val = read_img(style_img_path)\n",
    "'''\n",
    "The value of a feed cannot be a tf.Tensor object.\n",
    "需要先运行生成np.array\n",
    "'''\n",
    "content_img_op = tf.image.resize_bicubic(content_img_val,[224,224])\n",
    "style_img_op = tf.image.resize_bicubic(style_img_val,[224,224])\n",
    "\n",
    "\n",
    "content_img = tf.placeholder(tf.float32,shape=[1,224,224,3])\n",
    "style_img = tf.placeholder(tf.float32,shape=[1,224,224,3])\n",
    "\n",
    "# npy file 可以直接通过np进行读取 内容得到np.ndarray类型\n",
    "data_dict = np.load(vgg16_npy_path,allow_pickle=True,encoding=\"latin1\").item() # 转成字典类型\n",
    "'''\n",
    "其实也可以只构建一个网络\n",
    "content style 送入网络运算得到结果 \n",
    "之后直接与常结果计算loss 从而实现输入var的更新\n",
    "这里使用三个网络 因为构建本身耗时较少 而且更方便\n",
    "'''\n",
    "vggnet_for_content = VGGNet(data_dict)\n",
    "vggnet_for_style = VGGNet(data_dict)\n",
    "vggnet_for_result = VGGNet(data_dict)\n",
    "\n",
    "vggnet_for_content.build(content_img)\n",
    "vggnet_for_style.build(style_img)\n",
    "vggnet_for_result.build(result_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### style transfer kernel model (loss calculate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512, 512)\n",
      "(1, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "'''计算loss'''\n",
    "\n",
    "'''feature_size, [1, width, height, channel]'''\n",
    "content_features = [\n",
    "    vggnet_for_content.conv1_2,\n",
    "    # vgg_for_content.conv2_2,\n",
    "    # vgg_for_content.conv3_3,\n",
    "    # vgg_for_content.conv4_3,\n",
    "    # vgg_for_content.conv5_3\n",
    "]\n",
    "style_features = [\n",
    "    # vgg_for_style.conv1_2,\n",
    "    # vgg_for_style.conv2_2,\n",
    "    # vgg_for_style.conv3_3,\n",
    "    vggnet_for_style.conv4_3,\n",
    "    # vgg_for_style.conv5_3\n",
    "]\n",
    "result_content_features = [\n",
    "    vggnet_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    # vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "result_style_features = [\n",
    "    # vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    vggnet_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "def gram_matrix(x):\n",
    "    \"\"\"calculate style loss assess matrix\n",
    "    Args:\n",
    "    - x: features extracted from vggnet. shape=[1,w,h,ch]\n",
    "    \"\"\"\n",
    "    b,w,h,ch = x.get_shape().as_list()\n",
    "    x = tf.reshape(x,[b,h*w,ch]) # ?? 自动忽略第一维度??\n",
    "    '''[h*w, ch] matrix -> [ch, h*w] * [h*w, ch] -> [ch, ch]'''\n",
    "    gram = tf.matmul(x,x,adjoint_a=True) #对第一个matrix转置\n",
    "    gram = gram/tf.constant(ch*w*h,tf.float32) #tf类型敏感\n",
    "    return gram\n",
    "\n",
    "'''loss calculate - content loss'''\n",
    "''' two loss is variables '''\n",
    "content_loss = tf.zeros(1,tf.float32)\n",
    "for c,c_ in zip(content_features,result_content_features):\n",
    "    content_loss += tf.reduce_mean(tf.square(c-c_),axis = [1,2,3])\n",
    "\n",
    "style_loss = tf.zeros(1,tf.float32)\n",
    "'''get gram matrix to cal loss'''\n",
    "style_gram = [gram_matrix(i) for i in style_features]\n",
    "result_style_gram = [gram_matrix(i) for i in result_style_features]\n",
    "print(style_gram[0].get_shape())\n",
    "print(result_style_gram[0].get_shape())\n",
    "for s,s_ in zip(style_gram,result_style_gram):\n",
    "    style_loss += tf.reduce_mean(tf.square(s-s_),axis = [1,2])\n",
    "\n",
    "loss = content_loss*lambda_content + style_loss*lambda_style\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train pocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 7448.3359, content_loss: 48428.3672, style_loss:   5.2110\n",
      "step: 2, loss_value: 6688.9331, content_loss: 41532.7188, style_loss:   5.0713\n",
      "step: 3, loss_value: 5983.2725, content_loss: 35858.5820, style_loss:   4.7948\n",
      "step: 4, loss_value: 5389.6377, content_loss: 31515.6777, style_loss:   4.4761\n",
      "step: 5, loss_value: 4974.5527, content_loss: 28361.1855, style_loss:   4.2769\n",
      "step: 6, loss_value: 4699.3623, content_loss: 26158.2988, style_loss:   4.1671\n",
      "step: 7, loss_value: 4224.9463, content_loss: 24506.6641, style_loss:   3.5486\n",
      "step: 8, loss_value: 3736.1094, content_loss: 23300.8574, style_loss:   2.8120\n",
      "step: 9, loss_value: 3399.5518, content_loss: 22423.0176, style_loss:   2.3145\n",
      "step: 10, loss_value: 3137.6018, content_loss: 21768.0664, style_loss:   1.9216\n",
      "step: 11, loss_value: 2946.9092, content_loss: 21282.8477, style_loss:   1.6372\n",
      "step: 12, loss_value: 2812.5525, content_loss: 20899.6875, style_loss:   1.4452\n",
      "step: 13, loss_value: 2689.4094, content_loss: 20586.7070, style_loss:   1.2615\n",
      "step: 14, loss_value: 2596.1179, content_loss: 20293.4941, style_loss:   1.1335\n",
      "step: 15, loss_value: 2520.6250, content_loss: 19992.6250, style_loss:   1.0427\n",
      "step: 16, loss_value: 2447.1292, content_loss: 19672.1309, style_loss:   0.9598\n",
      "step: 17, loss_value: 2380.7983, content_loss: 19323.6895, style_loss:   0.8969\n",
      "step: 18, loss_value: 2317.6567, content_loss: 18954.6855, style_loss:   0.8444\n",
      "step: 19, loss_value: 2249.8901, content_loss: 18570.0508, style_loss:   0.7858\n",
      "step: 20, loss_value: 2187.7966, content_loss: 18168.1914, style_loss:   0.7420\n",
      "step: 21, loss_value: 2128.5098, content_loss: 17755.5156, style_loss:   0.7059\n",
      "step: 22, loss_value: 2068.8633, content_loss: 17342.2148, style_loss:   0.6693\n",
      "step: 23, loss_value: 2011.8657, content_loss: 16933.2344, style_loss:   0.6371\n",
      "step: 24, loss_value: 1959.6277, content_loss: 16532.5684, style_loss:   0.6127\n",
      "step: 25, loss_value: 1909.7114, content_loss: 16143.2881, style_loss:   0.5908\n",
      "step: 26, loss_value: 1862.7795, content_loss: 15765.0947, style_loss:   0.5725\n",
      "step: 27, loss_value: 1817.7495, content_loss: 15402.5957, style_loss:   0.5550\n",
      "step: 28, loss_value: 1774.2048, content_loss: 15057.1611, style_loss:   0.5370\n",
      "step: 29, loss_value: 1733.3398, content_loss: 14726.0391, style_loss:   0.5215\n",
      "step: 30, loss_value: 1694.5430, content_loss: 14410.9512, style_loss:   0.5069\n",
      "step: 31, loss_value: 1658.4514, content_loss: 14109.4346, style_loss:   0.4950\n",
      "step: 32, loss_value: 1625.0734, content_loss: 13822.6680, style_loss:   0.4856\n",
      "step: 33, loss_value: 1594.3455, content_loss: 13546.0117, style_loss:   0.4795\n",
      "step: 34, loss_value: 1566.0660, content_loss: 13285.9365, style_loss:   0.4749\n",
      "step: 35, loss_value: 1539.9390, content_loss: 13031.2002, style_loss:   0.4736\n",
      "step: 36, loss_value: 1508.2990, content_loss: 12796.1016, style_loss:   0.4574\n",
      "step: 37, loss_value: 1476.8370, content_loss: 12572.0576, style_loss:   0.4393\n",
      "step: 38, loss_value: 1449.9272, content_loss: 12360.4209, style_loss:   0.4278\n",
      "step: 39, loss_value: 1430.1019, content_loss: 12158.0088, style_loss:   0.4286\n",
      "step: 40, loss_value: 1417.7107, content_loss: 11963.7803, style_loss:   0.4427\n",
      "step: 41, loss_value: 1409.4745, content_loss: 11790.7568, style_loss:   0.4608\n",
      "step: 42, loss_value: 1388.5531, content_loss: 11621.7871, style_loss:   0.4527\n",
      "step: 43, loss_value: 1348.3477, content_loss: 11473.4609, style_loss:   0.4020\n",
      "step: 44, loss_value: 1342.3204, content_loss: 11337.3916, style_loss:   0.4172\n",
      "step: 45, loss_value: 1333.3193, content_loss: 11198.6055, style_loss:   0.4269\n",
      "step: 46, loss_value: 1303.6138, content_loss: 11072.7246, style_loss:   0.3927\n",
      "step: 47, loss_value: 1287.6321, content_loss: 10951.8721, style_loss:   0.3849\n",
      "step: 48, loss_value: 1279.0737, content_loss: 10825.9609, style_loss:   0.3930\n",
      "step: 49, loss_value: 1265.6099, content_loss: 10704.3193, style_loss:   0.3904\n",
      "step: 50, loss_value: 1244.1066, content_loss: 10592.7383, style_loss:   0.3697\n",
      "step: 51, loss_value: 1230.0651, content_loss: 10490.2412, style_loss:   0.3621\n",
      "step: 52, loss_value: 1220.4111, content_loss: 10377.1719, style_loss:   0.3654\n",
      "step: 53, loss_value: 1207.4692, content_loss: 10269.6084, style_loss:   0.3610\n",
      "step: 54, loss_value: 1196.5195, content_loss: 10171.4600, style_loss:   0.3587\n",
      "step: 55, loss_value: 1183.7865, content_loss: 10063.8623, style_loss:   0.3548\n",
      "step: 56, loss_value: 1169.1573, content_loss: 9966.2256, style_loss:   0.3451\n",
      "step: 57, loss_value: 1158.8473, content_loss: 9874.2422, style_loss:   0.3428\n",
      "step: 58, loss_value: 1146.2581, content_loss: 9779.2266, style_loss:   0.3367\n",
      "step: 59, loss_value: 1136.5920, content_loss: 9683.8760, style_loss:   0.3364\n",
      "step: 60, loss_value: 1133.2253, content_loss: 9595.1182, style_loss:   0.3474\n",
      "step: 61, loss_value: 1137.3440, content_loss: 9503.8008, style_loss:   0.3739\n",
      "step: 62, loss_value: 1149.3463, content_loss: 9427.6953, style_loss:   0.4132\n",
      "step: 63, loss_value: 1132.0374, content_loss: 9357.6738, style_loss:   0.3925\n",
      "step: 64, loss_value: 1119.6208, content_loss: 9300.0859, style_loss:   0.3792\n",
      "step: 65, loss_value: 1118.9636, content_loss: 9235.4707, style_loss:   0.3908\n",
      "step: 66, loss_value: 1105.0867, content_loss: 9187.3467, style_loss:   0.3727\n",
      "step: 67, loss_value: 1096.4628, content_loss: 9152.3311, style_loss:   0.3625\n",
      "step: 68, loss_value: 1080.6376, content_loss: 9101.8447, style_loss:   0.3409\n",
      "step: 69, loss_value: 1074.8053, content_loss: 9045.8057, style_loss:   0.3404\n",
      "step: 70, loss_value: 1065.0283, content_loss: 8992.0586, style_loss:   0.3316\n",
      "step: 71, loss_value: 1058.8912, content_loss: 8932.8438, style_loss:   0.3312\n",
      "step: 72, loss_value: 1052.6685, content_loss: 8873.1777, style_loss:   0.3307\n",
      "step: 73, loss_value: 1041.6121, content_loss: 8804.7988, style_loss:   0.3223\n",
      "step: 74, loss_value: 1037.6830, content_loss: 8740.9863, style_loss:   0.3272\n",
      "step: 75, loss_value: 1032.0394, content_loss: 8666.6670, style_loss:   0.3307\n",
      "step: 76, loss_value: 1023.3775, content_loss: 8607.3359, style_loss:   0.3253\n",
      "step: 77, loss_value: 1020.6415, content_loss: 8546.2178, style_loss:   0.3320\n",
      "step: 78, loss_value: 1004.0912, content_loss: 8481.9375, style_loss:   0.3118\n",
      "step: 79, loss_value: 998.9678, content_loss: 8411.8711, style_loss:   0.3156\n",
      "step: 80, loss_value: 989.5200, content_loss: 8357.6348, style_loss:   0.3075\n",
      "step: 81, loss_value: 978.5872, content_loss: 8292.1182, style_loss:   0.2988\n",
      "step: 82, loss_value: 978.2171, content_loss: 8226.1172, style_loss:   0.3112\n",
      "step: 83, loss_value: 970.4545, content_loss: 8175.6880, style_loss:   0.3058\n",
      "step: 84, loss_value: 975.3087, content_loss: 8110.4082, style_loss:   0.3285\n",
      "step: 85, loss_value: 991.9562, content_loss: 8052.4043, style_loss:   0.3734\n",
      "step: 86, loss_value: 976.3639, content_loss: 8004.9180, style_loss:   0.3517\n",
      "step: 87, loss_value: 961.5096, content_loss: 7971.3311, style_loss:   0.3288\n",
      "step: 88, loss_value: 958.4713, content_loss: 7922.7476, style_loss:   0.3324\n",
      "step: 89, loss_value: 953.2886, content_loss: 7876.5693, style_loss:   0.3313\n",
      "step: 90, loss_value: 937.5586, content_loss: 7842.0503, style_loss:   0.3067\n",
      "step: 91, loss_value: 934.1079, content_loss: 7798.9160, style_loss:   0.3084\n",
      "step: 92, loss_value: 933.1559, content_loss: 7743.9165, style_loss:   0.3175\n",
      "step: 93, loss_value: 917.2747, content_loss: 7689.3643, style_loss:   0.2967\n",
      "step: 94, loss_value: 914.6256, content_loss: 7639.4004, style_loss:   0.3014\n",
      "step: 95, loss_value: 905.2429, content_loss: 7586.7627, style_loss:   0.2931\n",
      "step: 96, loss_value: 897.2772, content_loss: 7536.5840, style_loss:   0.2872\n",
      "step: 97, loss_value: 894.4899, content_loss: 7483.3848, style_loss:   0.2923\n",
      "step: 98, loss_value: 886.6843, content_loss: 7419.4102, style_loss:   0.2895\n",
      "step: 99, loss_value: 889.4550, content_loss: 7360.7256, style_loss:   0.3068\n",
      "step: 100, loss_value: 907.7472, content_loss: 7316.9565, style_loss:   0.3521\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    content_img_val ,style_img_val = sess.run([content_img_op,style_img_op])\n",
    "#     (1, 224, 224, 3) <class 'numpy.ndarray'>\n",
    "    '''\n",
    "    print(content_img_val.shape,type(content_img_val))\n",
    "    print(content_img_val)\n",
    "    不转换问题在于不是float32 是float其他 所以需要修正\n",
    "    int32可以自动转换为float32 在tf中\n",
    "    '''\n",
    "    content_img_val = np.asarray(content_img_val,np.int32)\n",
    "    style_img_val = np.asarray(style_img_val,np.int32)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        loss_value,content_loss_value,style_loss_value,_ = sess.run([loss,content_loss,style_loss,train_op],\n",
    "                  feed_dict = {\n",
    "                      content_img:content_img_val,\n",
    "                      style_img:style_img_val\n",
    "                  })\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' \\\n",
    "            % (step+1,\n",
    "               loss_value[0],\n",
    "               content_loss_value[0],\n",
    "               style_loss_value[0]))\n",
    "        # save img\n",
    "        result_img_path = os.path.join(out_dir,'result-%04d.jpg'%(step+1))\n",
    "        '''photo prepocessing'''\n",
    "        result_img_val = result_img.eval(sess)[0]\n",
    "        result_img_val = np.clip(result_img_val,0,255)\n",
    "        result_img_val = np.asarray(result_img_val,np.uint8)\n",
    "        '''PIL Image save photo'''\n",
    "        img = Image.fromarray(result_img_val)\n",
    "        img.save(result_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## observe vgg16 param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, **k, allow_pickle=True)\n",
    "'''\n",
    "'''\n",
    "np.load 防止sql注入不支持默认导入 编码也要小心\n",
    "https://www.cnblogs.com/Tom-Ren/p/11054596.html\n",
    "如果使用python3读取python2生成的npy就有可能产生编码错误\n",
    "'ASCII', 'latin1', 'bytes'\n",
    "'''\n",
    "data = np.load(vgg16_npy_path,allow_pickle=True,encoding=\"latin1\")\n",
    "print(type(data))\n",
    "# print(data)\n",
    "data_dict = data.item()\n",
    "print(data_dict.keys())\n",
    "print(len(data_dict))\n",
    "conv1_1 = data_dict['conv1_1']\n",
    "print(len(conv1_1))\n",
    "w, b = conv1_1\n",
    "print(w.shape)\n",
    "print(b.shape)\n",
    "\n",
    "fc6 = data_dict['fc6']\n",
    "print(len(fc6))\n",
    "w, b = fc6\n",
    "print(w.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
