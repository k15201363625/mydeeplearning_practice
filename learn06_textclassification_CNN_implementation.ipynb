{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-text-classification\n",
    "- 调试须知：python中的函数不到调用不会检查 即便有拼写等错误"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepocessiong\n",
    "- 分词\n",
    "- 词语 -> id\n",
    "    - 由于：matrix -> [|V|, embed_size]\n",
    "    - 词表 从而得到词语的id表示(词语A -> id(5))\n",
    "- label -> id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "train_file = './text_classification/cnews.train.txt'\n",
    "val_file = './text_classification/cnews.val.txt'\n",
    "test_file = './text_classification/cnews.test.txt'\n",
    "#output files\n",
    "seg_train_file = './text_classification/cnews.train.seg.txt'\n",
    "seg_val_file = './text_classification/cnews.val.seg.txt'\n",
    "seg_test_file = './text_classification/cnews.test.seg.txt'\n",
    "\n",
    "vocab_file = './text_classification/cnews.vocab.txt'\n",
    "category_file = './text_classification/cnews.category.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### file read demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\gmm\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鲍勃库西奖归谁属？ NCAA最强控卫是坎巴还是弗神新浪体育讯如今，本赛季的NCAA进入到了末段，各项奖项的评选结果也即将出炉，其中评选最佳控卫的鲍勃-库西奖就将在下周最终四强战时公布，鲍勃-库西奖是由奈史密斯篮球名人堂提供，旨在奖励年度最佳大学控卫。最终获奖的球员也即将在以下几名热门人选中产生。〈〈〈 NCAA疯狂三月专题主页上线，点击链接查看精彩内容吉梅尔-弗雷戴特，杨百翰大学“弗神”吉梅尔-弗雷戴特一直都备受关注，他不仅仅是一名射手，他会用“终结对手脚踝”一样的变向过掉面前的防守者，并且他可以用任意一支手完成得分，如果他被犯规了，可以提前把这两份划入他的帐下了，因为他是一名命中率高达90%的罚球手。弗雷戴特具有所有伟大控卫都具备的一点特质，他是一位赢家也是一位领导者。“他整个赛季至始至终的稳定领导着球队前进，这是无可比拟的。”杨百翰大学主教练戴夫-罗斯称赞道，“他的得分能力毋庸置疑，但是我认为他带领球队获胜的能力才是他最重要的控卫职责。我们在主场之外的比赛(客场或中立场)共取胜19场，他都表现的很棒。”弗雷戴特能否在NBA取得成功？当然，但是有很多专业人士比我们更有资格去做出这样的判断。“我喜爱他。”凯尔特人主教练多克-里弗斯说道，“他很棒，我看过ESPN的片段剪辑，从剪辑来看，他是个超级巨星，我认为他很成为一名优秀的NBA球员。”诺兰-史密斯，杜克大学当赛季初，球队宣布大一天才控卫凯瑞-厄尔文因脚趾的伤病缺席赛季大部分比赛后，诺兰-史密斯便开始接管球权，他在进攻端上足发条，在ACC联盟(杜克大学所在分区)的得分榜上名列前茅，但同时他在分区助攻榜上也占据头名，这在众强林立的ACC联盟前无古人。“我不认为全美有其他的球员能在凯瑞-厄尔文受伤后，如此好的接管球队，并且之前毫无准备。”杜克主教练迈克-沙舍夫斯基赞扬道，“他会将比赛带入自己的节奏，得分，组织，领导球队，无所不能。而且他现在是攻防俱佳，对持球人的防守很有提高。总之他拥有了辉煌的赛季。”坎巴-沃克，康涅狄格大学坎巴-沃克带领康涅狄格在赛季初的毛伊岛邀请赛一路力克密歇根州大和肯塔基等队夺冠，他场均30分4助攻得到最佳球员。在大东赛区锦标赛和全国锦标赛中，他场均27.1分，6.1个篮板，5.1次助攻，依旧如此给力。他以疯狂的表现开始这个赛季，也将以疯狂的表现结束这个赛季。“我们在全国锦标赛中前进着，并且之前曾经5天连赢5场，赢得了大东赛区锦标赛的冠军，这些都归功于坎巴-沃克。”康涅狄格大学主教练吉姆-卡洪称赞道，“他是一名纯正的控卫而且能为我们得分，他有过单场42分，有过单场17助攻，也有过单场15篮板。这些都是一名6英尺175镑的球员所完成的啊！我们有很多好球员，但他才是最好的领导者，为球队所做的贡献也是最大。”乔丹-泰勒，威斯康辛大学全美没有一个持球者能像乔丹-泰勒一样很少失误，他4.26的助攻失误在全美遥遥领先，在大十赛区的比赛中，他平均35.8分钟才会有一次失误。他还是名很出色的得分手，全场砍下39分击败印第安纳大学的比赛就是最好的证明，其中下半场他曾经连拿18分。“那个夜晚他证明自己值得首轮顺位。”当时的见证者印第安纳大学主教练汤姆-克雷恩说道。“对一名控卫的所有要求不过是领导球队、使球队变的更好、带领球队成功，乔丹-泰勒全做到了。”威斯康辛教练博-莱恩说道。诺里斯-科尔，克利夫兰州大诺里斯-科尔的草根传奇正在上演，默默无闻的他被克利夫兰州大招募后便开始刻苦地训练，去年夏天他曾加练上千次跳投，来提高这个可能的弱点。他在本赛季与杨斯顿州大的比赛中得到40分20篮板和9次助攻，在他之前，过去15年只有一位球员曾经在NCAA一级联盟做到过40+20，他的名字是布雷克-格里芬。“他可以很轻松地防下对方王牌。”克利夫兰州大主教练加里-沃特斯如此称赞自己的弟子，“同时他还能得分，并为球队助攻，他几乎能做到一个成功的团队所有需要的事。”这其中四名球员都带领自己的球队进入到了甜蜜16强，虽然有3个球员和他们各自的球队被挡在8强的大门之外，但是他们已经表现的足够出色，不远的将来他们很可能出现在一所你熟悉的NBA球馆里。(clay)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.688 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鲍勃 库西 奖归 谁 属 ？   NCAA 最强 控卫 是 坎巴 还是 弗神 新浪 体育讯 如今 ， 本赛季 的 NCAA 进入 到 了 末段 ， 各项 奖项 的 评选 结果 也 即将 出炉 ， 其中 评选 最佳 控卫 的 鲍勃 - 库西 奖 就 将 在 下周 最终 四强 战时 公布 ， 鲍勃 - 库西 奖是 由奈 史密斯 篮球 名人堂 提供 ， 旨在 奖励 年度 最佳 大学 控卫 。 最终 获奖 的 球员 也 即将 在 以下 几名 热门 人选 中 产生 。 〈 〈 〈   NCAA 疯狂 三月 专题 主页 上线 ， 点击 链接 查看 精彩内容 吉梅尔 - 弗雷 戴特 ， 杨百翰 大学 “ 弗神 ” 吉梅尔 - 弗雷 戴特 一直 都 备受 关注 ， 他 不仅仅 是 一名 射手 ， 他会用 “ 终结 对手 脚踝 ” 一样 的 变向 过 掉 面前 的 防守 者 ， 并且 他 可以 用 任意 一支 手 完成 得分 ， 如果 他 被 犯规 了 ， 可以 提前 把 这 两份 划入 他 的 帐 下 了 ， 因为 他 是 一名 命中率 高达 90% 的 罚球 手 。 弗雷 戴特 具有 所有 伟大 控卫 都 具备 的 一点 特质 ， 他 是 一位 赢家 也 是 一位 领导者 。 “ 他 整个 赛季 至始 至终 的 稳定 领导 着 球队 前进 ， 这是 无可比拟 的 。 ” 杨百翰 大学 主教练 戴夫 - 罗斯 称赞 道 ， “ 他 的 得分 能力 毋庸置疑 ， 但是 我 认为 他 带领 球队 获胜 的 能力 才 是 他 最 重要 的 控卫 职责 。 我们 在 主场 之外 的 比赛 ( 客场 或 中 立场 ) 共 取胜 19 场 ， 他 都 表现 的 很棒 。 ” 弗雷 戴特 能否 在 NBA 取得成功 ？ 当然 ， 但是 有 很多 专业人士 比 我们 更 有 资格 去 做出 这样 的 判断 。 “ 我 喜爱 他 。 ” 凯尔特人 主教练 多克 - 里 弗斯 说道 ， “ 他 很棒 ， 我 看过 ESPN 的 片段 剪辑 ， 从 剪辑 来看 ， 他 是 个 超级 巨星 ， 我 认为 他 很 成为 一名 优秀 的 NBA 球员 。 ” 诺兰 - 史密斯 ， 杜克大学 当 赛季 初 ， 球队 宣布 大 一天 才 控卫凯瑞 - 厄尔 文因 脚趾 的 伤病 缺席 赛季 大部分 比赛 后 ， 诺兰 - 史密斯 便 开始 接管 球权 ， 他 在 进攻 端上 足 发条 ， 在 ACC 联盟 ( 杜克大学 所在 分区 ) 的 得分 榜上 名列前茅 ， 但 同时 他 在 分区 助攻 榜上 也 占据 头名 ， 这 在 众强 林立 的 ACC 联盟 前无古人 。 “ 我 不 认为 全美 有 其他 的 球员 能 在 凯瑞 - 厄尔 文 受伤 后 ， 如此 好 的 接管 球队 ， 并且 之前 毫无准备 。 ” 杜克 主教练 迈克 - 沙舍 夫斯基 赞扬 道 ， “ 他会 将 比赛 带入 自己 的 节奏 ， 得分 ， 组织 ， 领导 球队 ， 无所不能 。 而且 他 现在 是 攻防 俱佳 ， 对 持球 人 的 防守 很 有 提高 。 总之 他 拥有 了 辉煌 的 赛季 。 ” 坎巴 - 沃克 ， 康涅狄格 大学 坎巴 - 沃克 带领 康涅狄格 在 赛季 初 的 毛伊岛 邀请赛 一路 力克 密歇根州 大 和 肯塔基 等队 夺冠 ， 他场 均 30 分 4 助攻 得到 最佳 球员 。 在 大东 赛区 锦标赛 和 全国 锦标赛 中 ， 他场 均 27.1 分 ， 6.1 个 篮板 ， 5.1 次 助攻 ， 依旧 如此 给力 。 他 以 疯狂 的 表现 开始 这个 赛季 ， 也 将 以 疯狂 的 表现 结束 这个 赛季 。 “ 我们 在 全国 锦标赛 中 前进 着 ， 并且 之前 曾经 5 天 连赢 5 场 ， 赢得 了 大东 赛区 锦标赛 的 冠军 ， 这些 都 归功于 坎巴 - 沃克 。 ” 康涅狄格 大学 主教练 吉姆 - 卡洪 称赞 道 ， “ 他 是 一名 纯正 的 控卫 而且 能为 我们 得分 ， 他 有 过 单场 42 分 ， 有过 单场 17 助攻 ， 也 有 过 单场 15 篮板 。 这些 都 是 一名 6 英尺 175 镑 的 球员 所 完成 的 啊 ！ 我们 有 很多 好 球员 ， 但 他 才 是 最好 的 领导者 ， 为 球队 所 做 的 贡献 也 是 最大 。 ” 乔丹 - 泰勒 ， 威斯康辛 大学 全美 没有 一个 持球者 能 像 乔丹 - 泰勒 一样 很少 失误 ， 他 4.26 的 助攻 失误 在 全美 遥遥领先 ， 在 大十 赛区 的 比赛 中 ， 他 平均 35.8 分钟 才 会 有 一次 失误 。 他 还是 名 很 出色 的 得分手 ， 全场 砍 下 39 分 击败 印第安纳 大学 的 比赛 就是 最好 的 证明 ， 其中 下半场 他 曾经 连 拿 18 分 。 “ 那个 夜晚 他 证明 自己 值得 首轮 顺位 。 ” 当时 的 见证者 印第安纳 大学 主教练 汤姆 - 克 雷恩 说道 。 “ 对 一名 控卫 的 所有 要求 不过 是 领导 球队 、 使 球队 变 的 更好 、 带领 球队 成功 ， 乔丹 - 泰勒 全 做到 了 。 ” 威斯康辛 教练 博 - 莱恩 说道 。 诺里斯 - 科尔 ， 克利夫兰 州 大 诺里斯 - 科尔 的 草根 传奇 正在 上演 ， 默默无闻 的 他 被 克利夫兰 州 大 招募 后 便 开始 刻苦 地 训练 ， 去年 夏天 他 曾 加练 上 千次 跳投 ， 来 提高 这个 可能 的 弱点 。 他 在 本赛季 与 杨斯顿 州 大 的 比赛 中 得到 40 分 20 篮板 和 9 次 助攻 ， 在 他 之前 ， 过去 15 年 只有 一位 球员 曾经 在 NCAA 一级 联盟 做到 过 40 + 20 ， 他 的 名字 是 布雷克 - 格里芬 。 “ 他 可以 很 轻松 地防下 对方 王牌 。 ” 克利夫兰 州 大 主教练 加里 - 沃特斯 如此 称赞 自己 的 弟子 ， “ 同时 他 还 能 得分 ， 并 为 球队 助攻 ， 他 几乎 能 做到 一个 成功 的 团队 所有 需要 的 事 。 ” 这 其中 四名 球员 都 带领 自己 的 球队 进入 到 了 甜蜜 16 强 ， 虽然 有 3 个 球员 和 他们 各自 的 球队 被 挡 在 8 强 的 大门 之外 ， 但是 他们 已经 表现 的 足够 出色 ， 不远 的 将来 他们 很 可能 出现 在 一所 你 熟悉 的 NBA 球馆 里 。 ( clay )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''通过bytes-like对象读取\n",
    "with open(test_file,'rb') as f:\n",
    "    lines = f.readlines()\n",
    "label,content = lines[0].decode('utf-8').strip('\\r\\n').split('\\t')\n",
    "'''\n",
    "'''直接在读取的时候指定读取的编码'''\n",
    "with open(test_file,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "label,content = lines[0].strip('\\r\\n').split('\\t')\n",
    "'''如果以上两种都不用 报错 'gbk' codec can't decode意思是无法通过gbk进行解码\n",
    "原因是文本本身不是gbk 所以无法用默认的gbk方式解码 \n",
    "所以最直接的通过相应格式进行文件读取\n",
    "/使用读取字节文件方式读取 之后再对读取内容进行解码\n",
    "'''\n",
    "words = jieba.cut(content)\n",
    "print(content)\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 断句 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_file(input_file,output_file):\n",
    "    '''segment the sentences in each line from the inputfile'''\n",
    "    with open(input_file,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(output_file,'w',encoding='utf-8') as f:\n",
    "        for line in lines:\n",
    "            label,content = line.strip('\\r\\n').split('\\t')\n",
    "            words = jieba.cut(content)\n",
    "            content_by_word = ''\n",
    "            for word in words:\n",
    "                word.strip(' ')\n",
    "                if word != '':\n",
    "                    content_by_word += word+' '\n",
    "            out_line = '%s\\t%s\\n'%(label,content_by_word.strip(' '))\n",
    "            f.write(out_line)\n",
    "segment_file(train_file,seg_train_file)\n",
    "segment_file(val_file,seg_val_file)\n",
    "segment_file(test_file,seg_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词表 类表\n",
    "- 行号索引是id\n",
    "- 后面接上次数 便于后续处理 以及观察分布信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab_file(input_seg_file,output_vocab_file):\n",
    "    with open(input_seg_file,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    word_dict = {} # 统计词频\n",
    "    for line in lines:\n",
    "        label,content = line.strip('\\r\\n').split('\\t')\n",
    "        for word in content.split():\n",
    "            # 如果没有这个key设置为0\n",
    "            word_dict.setdefault(word,0)\n",
    "            word_dict[word] += 1 \n",
    "    # 按照频率进行排序 使用lambda函数对于元祖列表排序制定关键字\n",
    "    sorted_word_dict = sorted(word_dict.items(),\n",
    "                             key=lambda word:word[1],reverse=True)\n",
    "    with open(output_vocab_file,'w',encoding='utf-8') as f:\n",
    "        '''\n",
    "        写入未知字符标识\n",
    "        1:对于舍掉不常用的词可以再train中使用这个作为id\n",
    "        2:对于val / test中出现的train中没有的可以用这个作为id\n",
    "        3:对于不足threshold长度的sentence可以使用这个进行补齐\n",
    "        '''\n",
    "        f.write('<UNK>\\t100000000\\n')\n",
    "        for item in sorted_word_dict:\n",
    "            f.write('%s\\t%d\\n' % item)\n",
    "generate_vocab_file(seg_train_file,vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育\t5000\n",
      "\n",
      "娱乐\t5000\n",
      "\n",
      "家居\t5000\n",
      "\n",
      "房产\t5000\n",
      "\n",
      "教育\t5000\n",
      "\n",
      "时尚\t5000\n",
      "\n",
      "时政\t5000\n",
      "\n",
      "游戏\t5000\n",
      "\n",
      "科技\t5000\n",
      "\n",
      "财经\t5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_category_file(input_file,output_category_file):\n",
    "    with open(input_file,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    category_dict = {} # 统计词频\n",
    "    for line in lines:\n",
    "        label,content = line.strip('\\r\\n').split('\\t')\n",
    "        category_dict.setdefault(label,0)\n",
    "        category_dict[label] += 1 \n",
    "    # 按照频率进行排序 使用lambda函数对于元祖列表排序制定关键字\n",
    "    sorted_category_dict = sorted(category_dict.items(),\n",
    "                             key=lambda c:c[1],reverse=True)\n",
    "    with open(output_category_file,'w',encoding='utf-8') as f:\n",
    "        for item in sorted_category_dict:\n",
    "            print('%s\\t%d\\n' % item)\n",
    "            f.write('%s\\n' % item[0])\n",
    "            \n",
    "generate_category_file(train_file,category_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集封装+超参数定义\n",
    "- next_batch api \n",
    "    - read seg file get sentences id and category id by vocab and category class\n",
    "- 词表封装\n",
    "    - wordtoid api\n",
    "- 类别表封装\n",
    "    - categorytoid api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数定义(by HParams+tf logging output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "def get_default_param():\n",
    "    \"\"\"\n",
    "    return HParams object to get hyper parameters\n",
    "    可以通过构造方法传入参数 通过对象直接访问参数\n",
    "    \"\"\"\n",
    "    '''cnn 一维卷积特有的\n",
    "    - 指定输出通道数目 由filter数目决定\n",
    "    - 可定义多个 一维卷积kernel规模\n",
    "    '''\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_embedding_size = 16,#embedding length\n",
    "        num_timesteps = 50, #必须有\n",
    "        \n",
    "        num_filters = [64,],\n",
    "        num_kernel_size = [3,],\n",
    "        num_fc_nodes = 32,\n",
    "        batch_size = 100,\n",
    "        learning_rate = 1e-3,\n",
    "        num_word_threshold = 20,#超过这个长度额截断 不足补齐\n",
    "        # 防止梯度爆炸 梯度消失可以用lstm特性避免一部分\n",
    "    )\n",
    "hps = get_default_param()\n",
    "\n",
    "# 定义需要用到的w文件\n",
    "train_seg_file = './text_classification/cnews.train.seg.txt'\n",
    "val_seg_file = './text_classification/cnews.val.seg.txt'\n",
    "test_seg_file = './text_classification/cnews.test.seg.txt'\n",
    "vocab_file = './text_classification/cnews.vocab.txt'\n",
    "category_file = './text_classification/cnews.category.txt'\n",
    "\n",
    "output_folder = './text_classification/run_text_rnn'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词表封装\n",
    "- 实现sentencetoid api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:classes_num is:10\n",
      "\n",
      "INFO:tensorflow:label is:体育\t id is:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self,filename,num_word_threshold):\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._unk = -1\n",
    "        self._word_to_id_dict = {}\n",
    "        self._read_file(filename)\n",
    "    def _read_file(self,filename):\n",
    "        with open(filename,'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word,freq = line.strip('\\r\\n').split('\\t')\n",
    "            freq = int(freq)\n",
    "            if freq < self._num_word_threshold:\n",
    "                continue\n",
    "            nowid = len(self._word_to_id_dict)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = nowid\n",
    "            self._word_to_id_dict[word] = nowid\n",
    "    def _word_to_id(self,word):\n",
    "        '''没有的返回unk的id'''\n",
    "        return self._word_to_id_dict.get(word,self._unk)\n",
    "    \n",
    "    # 使用property 可以像获取公开成员方式访问函数\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self._word_to_id_dict)\n",
    "    def sentence_to_id(self,seg_sentence):\n",
    "        word_ids = [self._word_to_id(w) for w in seg_sentence.split()]\n",
    "        return word_ids\n",
    "    \n",
    "class Category:\n",
    "    def __init__(self,filename):\n",
    "        self._category_to_id_dict = {}\n",
    "        self._read_file(filename)\n",
    "        \n",
    "    def _read_file(self,filename):\n",
    "        with open(filename,'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            nowid = len(self._category_to_id_dict)\n",
    "            self._category_to_id_dict[category] = nowid\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self._category_to_id_dict)\n",
    "    \n",
    "    def category_to_id(self,category_name):\n",
    "        if not category_name in self._category_to_id_dict.keys():\n",
    "            raise Exception(\n",
    "                \"%s is not in our category list\" % category_name)\n",
    "        return self._category_to_id_dict[category_name]\n",
    "    \n",
    "vocab = Vocab(vocab_file,hps.num_word_threshold)\n",
    "'''size 在后续的网络搭建中会用到'''\n",
    "vocab_size = vocab.get_size()\n",
    "\n",
    "category = Category(category_file)\n",
    "num_classes = category.get_size()\n",
    "\n",
    "tf.logging.info('classes_num is:%d\\n'%num_classes)\n",
    "demo_str = '体育'\n",
    "tf.logging.info('label is:%s\\t id is:%s\\n' % \\\n",
    "                (demo_str,category.category_to_id(demo_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loading data from ./text_classification/cnews.train.seg.txt\n",
      "INFO:tensorflow:loading data from ./text_classification/cnews.val.seg.txt\n",
      "INFO:tensorflow:loading data from ./text_classification/cnews.test.seg.txt\n",
      "(array([[ 5053,  1337,    22,     0,  5730,  2499,  2897,    45,   688,\n",
      "           52,  1296,   660,    78,   103,    23,  1166,    44,     1,\n",
      "          225,  5053,  4314,    22,  5049, 20815,   970,     2,  4509,\n",
      "         5730,    50, 21777,  7485,     1,   508,  5544,  4057, 12425,\n",
      "            1,  1067,   562,  6199,     2,     8, 11164,  2942,     9,\n",
      "            3,  1199,  2942,  3033,  8393]]), array([4]))\n",
      "(array([[19061, 13982,    43, 45285, 19061,   273,  3545,  7416,    76,\n",
      "            0,  2144,     1,  4020,   579,    15,   602,   883, 26042,\n",
      "           57,     4, 19061,   527,   271, 15235,    14,     1,   161,\n",
      "          987,    22,   113,    94,    23,    75,  2189, 10031,     3,\n",
      "           43,  2058,  7847,     1,  1155,  7387,   651,   248,   704,\n",
      "         1308,   355,   840,     1,    64]]), array([9]))\n",
      "(array([[  467,    11,   329, 10266, 15226,  3390, 10201, 17485, 49931,\n",
      "           12,  5704,  2320,    11, 10201,  2043,  2828,   112,  1042,\n",
      "            1,     4,   776,    10,   329, 14556, 10104,     3, 20906,\n",
      "        10201,  2168,  9751,     6,   405,   224,     3,   125,     4,\n",
      "          287,     2,   151,     1,  3390,  3746,   152,  3358,    20,\n",
      "           42,    48,  1640,     1,   287]]), array([5]))\n"
     ]
    }
   ],
   "source": [
    "class TextClassificationDataSet:\n",
    "    def __init__(self,filename,vocab,category,num_timesteps,need_shuffle=True):\n",
    "        \"\"\"\n",
    "        提供next_batch 并且需要通过num_timesteps 得到batch中的语句词语数目\n",
    "        便于对齐\n",
    "        \"\"\"\n",
    "        self._vocab = vocab\n",
    "        self._category = category\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._need_shuffle = need_shuffle\n",
    "        '''data is id matrix,label is id vector'''\n",
    "        self._datas = []\n",
    "        self._labels = []\n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "    def _parse_file(self,filename):\n",
    "        tf.logging.info('loading data from %s' % filename)\n",
    "        with open(filename,'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label,content = line.strip('\\r\\n').split('\\t')\n",
    "            id_label = self._category.category_to_id(label)\n",
    "            id_content = self._vocab.sentence_to_id(content)\n",
    "            '''进行对齐长度处理(尽可能少用if)'''\n",
    "            id_content = id_content[0:self._num_timesteps]\n",
    "            padding_num = self._num_timesteps - len(id_content)\n",
    "            id_content = id_content + [self._vocab.unk for i in range(padding_num)]\n",
    "            self._datas.append(id_content)\n",
    "            self._labels.append(id_label)\n",
    "        '''切记对于从文本中读取的数字是str 需要转换成int类型 ?在tf中可以自动转换为float32'''\n",
    "        self._datas = np.asarray(self._datas,dtype=np.int32)\n",
    "        self._labels = np.asarray(self._labels,dtype=np.int32)\n",
    "        '''首次生成需要random shuffle'''\n",
    "        self._random_shuffle()\n",
    "        \n",
    "    def next_batch(self,batch_size):\n",
    "        # 默认参数的值不能是init声明的成员变量吗??\n",
    "        if batch_size > len(self._datas):\n",
    "            raise Execption(\"batch_size: %d is too large\" % batch_size)        \n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._datas):\n",
    "            if self._need_shuffle:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        \n",
    "        batch_datas = self._datas[self._indicator:end_indicator]\n",
    "        batch_labels = self._labels[self._indicator:end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_datas,batch_labels\n",
    "    \n",
    "    def _random_shuffle(self):\n",
    "        idx = np.random.permutation(len(self._datas))\n",
    "        self._datas = self._datas[idx]\n",
    "        self._labels = self._labels[idx]\n",
    "\n",
    "train_dataset = TextClassificationDataSet(train_seg_file,vocab,\n",
    "                                          category,hps.num_timesteps)\n",
    "val_dataset = TextClassificationDataSet(val_seg_file,vocab,category,\n",
    "                                        hps.num_timesteps,False)\n",
    "test_dataset = TextClassificationDataSet(test_seg_file,vocab,category,\n",
    "                                         hps.num_timesteps,False)\n",
    "\n",
    "print(train_dataset.next_batch(1))\n",
    "print(val_dataset.next_batch(1))\n",
    "print(test_dataset.next_batch(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create model\n",
    "- 搭建cnn text classification model\n",
    "- 效果：更快收敛 每一步训练更快 效果也会更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''edition 2 手动实现lstmcell'''\n",
    "def create_model(hps,vocab_size,num_classes):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size   \n",
    "    inputs = tf.placeholder(tf.int32,(batch_size,num_timesteps))\n",
    "    outputs = tf.placeholder(tf.int32,(batch_size,))\n",
    "    keep_prob = tf.placeholder(tf.float32,name='dropout_keep_prob')\n",
    "    \n",
    "    '''input_layer'''\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0,1.0)\n",
    "    with tf.variable_scope('embedding',initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable('vocab_embedding_matrix',\n",
    "                                    [vocab_size,hps.num_embedding_size],\n",
    "                                    tf.float32)\n",
    "\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings,inputs)\n",
    "\n",
    "    '''cnn layer'''\n",
    "    scale = 1.0/math.sqrt(hps.num_embedding_size+hps.num_filters[0])/3.0\n",
    "    cnn_initializer = tf.random_uniform_initializer(-scale,scale)\n",
    "    with tf.variable_scope('cnn_layer',initializer=cnn_initializer):\n",
    "        '''\n",
    "        如果使用一种卷积kernel长度\n",
    "        embed_inputs: [batch_size, timesteps, embed_size]\n",
    "        conv1d: [batch_size, timesteps, num_filters]\n",
    "        如果多种长度kernel 处理方法类似\n",
    "        '''\n",
    "        conv1d = tf.layers.conv1d(embed_inputs,hps.num_filters[0],\n",
    "                                 hps.num_kernel_size[0],activation=tf.nn.relu)\n",
    "        '''时间维度pooling'''\n",
    "        global_maxpooling = tf.reduce_max(conv1d,axis=[1])\n",
    "        \n",
    "    '''fc layer + output layer'''\n",
    "    fc_initializer = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc_layer',initializer=fc_initializer):\n",
    "        fc1 = tf.layers.dense(global_maxpooling,hps.num_fc_nodes,\n",
    "                             activation=tf.nn.relu,name='fc1')\n",
    "        droped_fc1 = tf.contrib.layers.dropout(fc1,keep_prob)\n",
    "        logits = tf.layers.dense(droped_fc1,num_classes,name='fc2')\n",
    "        \n",
    "    with tf.name_scope('metrics'):\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            logits=logits,labels=outputs)\n",
    "        y_predict = tf.argmax(tf.nn.softmax(logits),\n",
    "                              axis=1,output_type=tf.int32) #小心类型 才能equal\n",
    "        correct_pred = tf.equal(outputs,y_predict)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "    global_step = tf.Variable(tf.zeros([],tf.int64),name='global_step',\n",
    "                             trainable=False)\n",
    "\n",
    "    with tf.name_scope('train_op'):\n",
    "        train_op = tf.train.AdamOptimizer(hps.learning_rate).minimize(\n",
    "            loss, global_step=global_step)\n",
    "    return ((inputs,outputs,keep_prob),\n",
    "           (loss,accuracy),\n",
    "           (train_op,global_step))    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "placeholder,metrics,others = create_model(hps,vocab_size,num_classes)\n",
    "\n",
    "inputs,outputs,keep_prob = placeholder\n",
    "loss,accuracy = metrics\n",
    "train_op,global_step = others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:     0, loss: 2.339, accuracy: 0.110\n",
      "INFO:tensorflow:Step:    50, loss: 2.256, accuracy: 0.220\n",
      "INFO:tensorflow:Step:   100, loss: 2.069, accuracy: 0.330\n",
      "INFO:tensorflow:Step:   150, loss: 1.601, accuracy: 0.470\n",
      "INFO:tensorflow:Step:   200, loss: 1.409, accuracy: 0.530\n",
      "INFO:tensorflow:Step:   250, loss: 1.254, accuracy: 0.600\n",
      "INFO:tensorflow:Step:   300, loss: 1.227, accuracy: 0.580\n",
      "INFO:tensorflow:Step:   350, loss: 1.082, accuracy: 0.650\n",
      "INFO:tensorflow:Step:   400, loss: 0.919, accuracy: 0.730\n",
      "INFO:tensorflow:Step:   450, loss: 0.886, accuracy: 0.710\n",
      "INFO:tensorflow:Step:   500, loss: 0.812, accuracy: 0.710\n",
      "INFO:tensorflow:Step:   550, loss: 0.783, accuracy: 0.760\n",
      "INFO:tensorflow:Step:   600, loss: 0.853, accuracy: 0.690\n",
      "INFO:tensorflow:Step:   650, loss: 0.597, accuracy: 0.810\n",
      "INFO:tensorflow:Step:   700, loss: 0.775, accuracy: 0.770\n",
      "INFO:tensorflow:Step:   750, loss: 0.680, accuracy: 0.800\n",
      "INFO:tensorflow:Step:   800, loss: 0.802, accuracy: 0.730\n",
      "INFO:tensorflow:Step:   850, loss: 0.582, accuracy: 0.820\n",
      "INFO:tensorflow:Step:   900, loss: 0.630, accuracy: 0.800\n",
      "INFO:tensorflow:Step:   950, loss: 0.462, accuracy: 0.870\n",
      "INFO:tensorflow:Step:  1000, loss: 0.654, accuracy: 0.820\n",
      "INFO:tensorflow:Step:  1050, loss: 0.506, accuracy: 0.860\n",
      "INFO:tensorflow:Step:  1100, loss: 0.359, accuracy: 0.920\n",
      "INFO:tensorflow:Step:  1150, loss: 0.379, accuracy: 0.890\n",
      "INFO:tensorflow:Step:  1200, loss: 0.522, accuracy: 0.860\n",
      "INFO:tensorflow:Step:  1250, loss: 0.498, accuracy: 0.870\n",
      "INFO:tensorflow:Step:  1300, loss: 0.424, accuracy: 0.870\n",
      "INFO:tensorflow:Step:  1350, loss: 0.320, accuracy: 0.920\n",
      "INFO:tensorflow:Step:  1400, loss: 0.448, accuracy: 0.850\n",
      "INFO:tensorflow:Step:  1450, loss: 0.573, accuracy: 0.820\n",
      "INFO:tensorflow:Step:  1500, loss: 0.500, accuracy: 0.850\n",
      "INFO:tensorflow:Step:  1550, loss: 0.378, accuracy: 0.830\n",
      "INFO:tensorflow:Step:  1600, loss: 0.383, accuracy: 0.900\n",
      "INFO:tensorflow:Step:  1650, loss: 0.416, accuracy: 0.920\n",
      "INFO:tensorflow:Step:  1700, loss: 0.520, accuracy: 0.870\n",
      "INFO:tensorflow:Step:  1750, loss: 0.351, accuracy: 0.920\n",
      "INFO:tensorflow:Step:  1800, loss: 0.422, accuracy: 0.860\n",
      "INFO:tensorflow:Step:  1850, loss: 0.408, accuracy: 0.870\n",
      "INFO:tensorflow:Step:  1900, loss: 0.233, accuracy: 0.930\n",
      "INFO:tensorflow:Step:  1950, loss: 0.354, accuracy: 0.920\n",
      "INFO:tensorflow:Step:  2000, loss: 0.299, accuracy: 0.910\n",
      "INFO:tensorflow:Step:  2050, loss: 0.237, accuracy: 0.920\n",
      "INFO:tensorflow:Step:  2100, loss: 0.341, accuracy: 0.920\n",
      "INFO:tensorflow:Step:  2150, loss: 0.455, accuracy: 0.880\n",
      "INFO:tensorflow:Step:  2200, loss: 0.278, accuracy: 0.910\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-4fabb8c1be2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                                  \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_datas\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                  \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                                  \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_keep_prob_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                              })\n\u001b[0;32m     18\u001b[0m         \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mglobal_step_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\environments\\mytf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\environments\\mytf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\environments\\mytf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\environments\\mytf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\environments\\mytf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\environments\\mytf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_val = 0.7\n",
    "test_keep_prob_val = 1.0\n",
    "num_train_steps = 10000\n",
    "# Train: 99.7%\n",
    "# Valid: 92.7%\n",
    "# Test:  93.2%\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_datas,batch_labels = train_dataset.next_batch(hps.batch_size)\n",
    "        output_val = sess.run([loss,accuracy,train_op,global_step],\n",
    "                             feed_dict={\n",
    "                                 inputs:batch_datas,\n",
    "                                 outputs:batch_labels,\n",
    "                                 keep_prob:train_keep_prob_val\n",
    "                             })\n",
    "        loss_val,accuracy_val,_,global_step_val = output_val\n",
    "        if global_step_val % 50 == 0:\n",
    "            tf.logging.info(\"Step: %5d, loss: %3.3f, accuracy: %3.3f\"\n",
    "                            % (global_step_val, loss_val, accuracy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "- 使用cnn架构虽然无法处理变长序列问题 但是可以更快 因为cnn并行计算远快于rnn\n",
    "- 目前为止已学到的模型\n",
    "    - rnn\n",
    "        - 单向lstm取最后输出\n",
    "        - 双向并对考虑所有输出\n",
    "        - hiarachical attention lstm 加入了层级关系 并且引入了注意力模型\n",
    "    - cnn(虽然应用场景受到限制 但是更快表现甚至更好)\n",
    "        - cnn 文本分类器(一维卷积)\n",
    "    - rnn+cnn\n",
    "        - 用rnn接受边长输入 得到输出作为embedding(取代原来直接就是输入的imbedding)\n",
    "        - 再用cnn进行处理 之后pooling + fc得到结果\n",
    "        - 结合二者优点 拓宽cnn使用场景\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
